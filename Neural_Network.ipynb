{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/lekhasmacbook/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/lekhasmacbook/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.neighbors import KNeighborsClassifier  \n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer,HashingVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_json(\"train.json\")\n",
    "testset = pd.read_json(\"test.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cuisine</th>\n",
       "      <th>id</th>\n",
       "      <th>ingredients</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>greek</td>\n",
       "      <td>10259</td>\n",
       "      <td>[romaine lettuce, black olives, grape tomatoes...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>southern_us</td>\n",
       "      <td>25693</td>\n",
       "      <td>[plain flour, ground pepper, salt, tomatoes, g...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>filipino</td>\n",
       "      <td>20130</td>\n",
       "      <td>[eggs, pepper, salt, mayonaise, cooking oil, g...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>indian</td>\n",
       "      <td>22213</td>\n",
       "      <td>[water, vegetable oil, wheat, salt]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>indian</td>\n",
       "      <td>13162</td>\n",
       "      <td>[black pepper, shallots, cornflour, cayenne pe...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       cuisine     id                                        ingredients\n",
       "0        greek  10259  [romaine lettuce, black olives, grape tomatoes...\n",
       "1  southern_us  25693  [plain flour, ground pepper, salt, tomatoes, g...\n",
       "2     filipino  20130  [eggs, pepper, salt, mayonaise, cooking oil, g...\n",
       "3       indian  22213                [water, vegetable oil, wheat, salt]\n",
       "4       indian  13162  [black pepper, shallots, cornflour, cayenne pe..."
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>ingredients</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>18009</td>\n",
       "      <td>[baking powder, eggs, all-purpose flour, raisi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>28583</td>\n",
       "      <td>[sugar, egg yolks, corn starch, cream of tarta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>41580</td>\n",
       "      <td>[sausage links, fennel bulb, fronds, olive oil...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>29752</td>\n",
       "      <td>[meat cuts, file powder, smoked sausage, okra,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>35687</td>\n",
       "      <td>[ground black pepper, salt, sausage casings, l...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      id                                        ingredients\n",
       "0  18009  [baking powder, eggs, all-purpose flour, raisi...\n",
       "1  28583  [sugar, egg yolks, corn starch, cream of tarta...\n",
       "2  41580  [sausage links, fennel bulb, fronds, olive oil...\n",
       "3  29752  [meat cuts, file powder, smoked sausage, okra,...\n",
       "4  35687  [ground black pepper, salt, sausage casings, l..."
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "cuisine        0\n",
       "id             0\n",
       "ingredients    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id             0\n",
       "ingredients    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testset.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check different types of cuisines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['greek', 'southern_us', 'filipino', 'indian', 'jamaican',\n",
       "       'spanish', 'italian', 'mexican', 'chinese', 'british', 'thai',\n",
       "       'vietnamese', 'cajun_creole', 'brazilian', 'french', 'japanese',\n",
       "       'irish', 'korean', 'moroccan', 'russian'], dtype=object)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.cuisine.unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Data processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert the ingredients to string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.ingredients = df.ingredients.astype('str')\n",
    "testset.ingredients = testset.ingredients.astype('str')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"['romaine lettuce', 'black olives', 'grape tomatoes', 'garlic', 'pepper', 'purple onion', 'seasoning', 'garbanzo beans', 'feta cheese crumbles']\""
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.ingredients[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"['baking powder', 'eggs', 'all-purpose flour', 'raisins', 'milk', 'white sugar']\""
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testset.ingredients[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lets remove those unnecessary symbols, which might be problem when tokenizing and lemmatizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.ingredients = df.ingredients.str.replace(\"[\",\" \")\n",
    "df.ingredients = df.ingredients.str.replace(\"]\",\" \")\n",
    "df.ingredients = df.ingredients.str.replace(\"'\",\" \")\n",
    "df.ingredients = df.ingredients.str.replace(\",\",\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "testset.ingredients = testset.ingredients.str.replace(\"[\",\" \")\n",
    "testset.ingredients = testset.ingredients.str.replace(\"]\",\" \")\n",
    "testset.ingredients = testset.ingredients.str.replace(\"'\",\" \")\n",
    "testset.ingredients = testset.ingredients.str.replace(\",\",\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'  romaine lettuce    black olives    grape tomatoes    garlic    pepper    purple onion    seasoning    garbanzo beans    feta cheese crumbles  '"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.ingredients[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'  baking powder    eggs    all-purpose flour    raisins    milk    white sugar  '"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testset.ingredients[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert everything to lower ( I think they are already in lower case, but to be on safe side)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.ingredients = df.ingredients.str.lower()\n",
    "testset.ingredients = testset.ingredients.str.lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets TOKENIZE the data now. (the processing of splitting into individual words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I', 'am', 'the', 'best']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_tokenize('I am the best')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.ingredients = df.ingredients.apply(lambda x: word_tokenize(x))\n",
    "testset.ingredients = testset.ingredients.apply(lambda x: word_tokenize(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    [romaine, lettuce, black, olives, grape, tomat...\n",
       "1    [plain, flour, ground, pepper, salt, tomatoes,...\n",
       "2    [eggs, pepper, salt, mayonaise, cooking, oil, ...\n",
       "3                 [water, vegetable, oil, wheat, salt]\n",
       "4    [black, pepper, shallots, cornflour, cayenne, ...\n",
       "Name: ingredients, dtype: object"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.ingredients[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    [baking, powder, eggs, all-purpose, flour, rai...\n",
       "1    [sugar, egg, yolks, corn, starch, cream, of, t...\n",
       "2    [sausage, links, fennel, bulb, fronds, olive, ...\n",
       "3    [meat, cuts, file, powder, smoked, sausage, ok...\n",
       "4    [ground, black, pepper, salt, sausage, casings...\n",
       "Name: ingredients, dtype: object"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testset.ingredients[0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets LEMMATIZE the data now (Since i believe that dataset might have different representation of same words, like the olives and olive, tomatoes and tomato, which represent the same word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmat(wor):\n",
    "    l = []\n",
    "    for i in wor:\n",
    "        l.append(lemmatizer.lemmatize(i))\n",
    "    return l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.ingredients = df.ingredients.apply(lemmat)\n",
    "testset.ingredients = testset.ingredients.apply(lemmat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['romaine',\n",
       " 'lettuce',\n",
       " 'black',\n",
       " 'olive',\n",
       " 'grape',\n",
       " 'tomato',\n",
       " 'garlic',\n",
       " 'pepper',\n",
       " 'purple',\n",
       " 'onion',\n",
       " 'seasoning',\n",
       " 'garbanzo',\n",
       " 'bean',\n",
       " 'feta',\n",
       " 'cheese',\n",
       " 'crumbles']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.ingredients[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['baking',\n",
       " 'powder',\n",
       " 'egg',\n",
       " 'all-purpose',\n",
       " 'flour',\n",
       " 'raisin',\n",
       " 'milk',\n",
       " 'white',\n",
       " 'sugar']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testset.ingredients[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observe that olives converted to olive, tomatoes to tomato etc, many words are now in their root form."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(df.ingredients[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lemmatization converted it back to list, so change to str again and remove the unncessary words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.ingredients = df.ingredients.astype('str')\n",
    "df.ingredients = df.ingredients.str.replace(\"[\",\" \")\n",
    "df.ingredients = df.ingredients.str.replace(\"]\",\" \")\n",
    "df.ingredients = df.ingredients.str.replace(\"'\",\" \")\n",
    "df.ingredients = df.ingredients.str.replace(\",\",\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "testset.ingredients = testset.ingredients.astype('str')\n",
    "testset.ingredients = testset.ingredients.str.replace(\"[\",\" \")\n",
    "testset.ingredients = testset.ingredients.str.replace(\"]\",\" \")\n",
    "testset.ingredients = testset.ingredients.str.replace(\"'\",\" \")\n",
    "testset.ingredients = testset.ingredients.str.replace(\",\",\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "str"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(df.ingredients[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'  romaine    lettuce    black    olive    grape    tomato    garlic    pepper    purple    onion    seasoning    garbanzo    bean    feta    cheese    crumbles  '"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.ingredients[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now our data looks good for vectorization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#vect = HashingVectorizer ()\n",
    "vect = TfidfVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = vect.fit_transform(df.ingredients)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<39774x2788 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 756011 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "scipy.sparse.csr.csr_matrix"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, **now our features has 2826 features, which are created by the process of vectorization.**\n",
    "\n",
    "Lets visualize some random features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "testfeatures = vect.transform(testset.ingredients)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<9944x2788 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 189396 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testfeatures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets create our labels now, which is obviously cuisine column. Lets labelencode it so that they convert to numerical lables, which usually might give better prediction results. Not a necessary step tho"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = LabelEncoder()\n",
    "labels = encoder.fit_transform(df.cuisine)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets split the dataset into training and testing parts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(features, labels, test_size=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the shapes, to make sure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(31819, 2788) (7955, 2788) (31819,) (7955,)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape, X_test.shape, y_train.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:758: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=10, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=400, multi_class='multinomial',\n",
       "          n_jobs=None, penalty='l2', random_state=None, solver='lbfgs',\n",
       "          tol=0.0001, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logreg = LogisticRegression(C=10,solver='lbfgs', multi_class='multinomial',max_iter=400)\n",
    "#C : float, default: 1.0; Inverse of regularization strength; must be a positive float. Like in support vector machines, smaller values specify stronger regularization.\n",
    "\n",
    "\n",
    "logreg.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression accuracy 0.7912005028284098\n"
     ]
    }
   ],
   "source": [
    "print(\"Logistic Regression accuracy\",logreg.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 3,  5, 12, ...,  9, 12,  9])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logreg.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/stochastic_gradient.py:166: FutureWarning: max_iter and tol parameters have been added in SGDClassifier in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SGDClassifier(alpha=0.0001, average=False, class_weight=None,\n",
       "       early_stopping=False, epsilon=0.1, eta0=0.0, fit_intercept=True,\n",
       "       l1_ratio=0.15, learning_rate='optimal', loss='hinge', max_iter=None,\n",
       "       n_iter=None, n_iter_no_change=5, n_jobs=None, penalty='l2',\n",
       "       power_t=0.5, random_state=None, shuffle=True, tol=None,\n",
       "       validation_fraction=0.1, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import linear_model\n",
    "sgd = linear_model.SGDClassifier()\n",
    "sgd.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This estimator implements regularized linear models with stochastic gradient descent (SGD) learning: the gradient of the loss is estimated each sample at a time and the model is updated along the way with a decreasing strength schedule (aka learning rate). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SGD classifier accuracy 0.7836580766813325\n"
     ]
    }
   ],
   "source": [
    "print(\"SGD classifier accuracy\",sgd.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/sklearn/svm/base.py:931: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LinearSVC(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "     intercept_scaling=1, loss='squared_hinge', max_iter=1500,\n",
       "     multi_class='crammer_singer', penalty='l2', random_state=0,\n",
       "     tol=0.0001, verbose=0)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "linearsvm = LinearSVC(C=1.0,random_state=0,multi_class='crammer_singer',dual = False, max_iter = 1500)\n",
    "linearsvm.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear SVM accuracy 0.7972344437460717\n"
     ]
    }
   ],
   "source": [
    "print(\"Linear SVM accuracy\", linearsvm.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, lets try our luck with neural networks.\n",
    "\n",
    "## NEURAL NETWORK'S\n",
    "\n",
    "I have tried both Keras and tensorflow (Of course the backend is same), but Keras code looks simpler and clear.\n",
    "\n",
    "For Neural Networks we need to have the dense array's as inputs and preferably one hot encoding for lables. So, lets create lables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "labelsNN = df.cuisine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert it to one hot formatting, there are many ways to do, i prefer to do this way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "labelsNN = pd.get_dummies(labelsNN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert it to arrays, you can do by values method or np.array() both are same"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "labelsNN = labelsNN.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's how the one hot encoding looks like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "      dtype=uint8)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labelsNN[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our labels are ready, now we need the features, **we have already created the features above but it was sparse matrix, which neural network doesnt like, so convert to dense arrays.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.sparse import csr_matrix\n",
    "sparse_dataset = csr_matrix(features)\n",
    "featuresNN = sparse_dataset.todense()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's how the features look like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[0., 0., 0., ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "featuresNN[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(31819, 2788) (7955, 2788) (31819, 20) (7955, 20)\n"
     ]
    }
   ],
   "source": [
    "X_trainNN, X_testNN, y_trainNN, y_testNN = train_test_split(featuresNN, labelsNN, test_size=0.2)\n",
    "print(X_trainNN.shape, X_testNN.shape, y_trainNN.shape, y_testNN.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2788\n"
     ]
    }
   ],
   "source": [
    "numfeat = X_trainNN.shape[1]\n",
    "print(numfeat)\n",
    "#The shape attribute for numpy arrays returns the dimensions of the array. \n",
    "#If Y has n rows and m columns, then Y.shape is (n,m). So Y.shape[1] is m."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KERAS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from keras.layers import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A sequential NN with 300,500 and 400 nodes in first,second and third layers resp.\n",
    "\n",
    "The loss is categorical cross entropy and the optimizer is adam with default learning rate. We can tweak a lot of parameters like the no of nodes, epochs, batchsize etc to improve accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      " - 6s - loss: 1.8799 - categorical_accuracy: 0.4583\n",
      "Epoch 2/50\n",
      " - 5s - loss: 0.9029 - categorical_accuracy: 0.7236\n",
      "Epoch 3/50\n",
      " - 5s - loss: 0.6730 - categorical_accuracy: 0.7972\n",
      "Epoch 4/50\n",
      " - 5s - loss: 0.5575 - categorical_accuracy: 0.8343\n",
      "Epoch 5/50\n",
      " - 5s - loss: 0.4757 - categorical_accuracy: 0.8562\n",
      "Epoch 6/50\n",
      " - 5s - loss: 0.4141 - categorical_accuracy: 0.8738\n",
      "Epoch 7/50\n",
      " - 5s - loss: 0.3534 - categorical_accuracy: 0.8942\n",
      "Epoch 8/50\n",
      " - 5s - loss: 0.2935 - categorical_accuracy: 0.9115\n",
      "Epoch 9/50\n",
      " - 5s - loss: 0.2380 - categorical_accuracy: 0.9301\n",
      "Epoch 10/50\n",
      " - 5s - loss: 0.1875 - categorical_accuracy: 0.9475\n",
      "Epoch 11/50\n",
      " - 5s - loss: 0.1410 - categorical_accuracy: 0.9615\n",
      "Epoch 12/50\n",
      " - 5s - loss: 0.1034 - categorical_accuracy: 0.9732\n",
      "Epoch 13/50\n",
      " - 5s - loss: 0.0759 - categorical_accuracy: 0.9818\n",
      "Epoch 14/50\n",
      " - 5s - loss: 0.0563 - categorical_accuracy: 0.9873\n",
      "Epoch 15/50\n",
      " - 5s - loss: 0.0405 - categorical_accuracy: 0.9919\n",
      "Epoch 16/50\n",
      " - 5s - loss: 0.0303 - categorical_accuracy: 0.9945\n",
      "Epoch 17/50\n",
      " - 5s - loss: 0.0212 - categorical_accuracy: 0.9967\n",
      "Epoch 18/50\n",
      " - 5s - loss: 0.0170 - categorical_accuracy: 0.9978\n",
      "Epoch 19/50\n",
      " - 5s - loss: 0.0141 - categorical_accuracy: 0.9982\n",
      "Epoch 20/50\n",
      " - 5s - loss: 0.0115 - categorical_accuracy: 0.9983\n",
      "Epoch 21/50\n",
      " - 5s - loss: 0.0099 - categorical_accuracy: 0.9989\n",
      "Epoch 22/50\n",
      " - 6s - loss: 0.0100 - categorical_accuracy: 0.9985\n",
      "Epoch 23/50\n",
      " - 6s - loss: 0.0102 - categorical_accuracy: 0.9986\n",
      "Epoch 24/50\n",
      " - 6s - loss: 0.0091 - categorical_accuracy: 0.9987\n",
      "Epoch 25/50\n",
      " - 7s - loss: 0.0104 - categorical_accuracy: 0.9987\n",
      "Epoch 26/50\n",
      " - 6s - loss: 0.0096 - categorical_accuracy: 0.9984\n",
      "Epoch 27/50\n",
      " - 5s - loss: 0.0091 - categorical_accuracy: 0.9986\n",
      "Epoch 28/50\n",
      " - 5s - loss: 0.0123 - categorical_accuracy: 0.9974\n",
      "Epoch 29/50\n",
      " - 5s - loss: 0.0279 - categorical_accuracy: 0.9928\n",
      "Epoch 30/50\n",
      " - 5s - loss: 0.0464 - categorical_accuracy: 0.9867\n",
      "Epoch 31/50\n",
      " - 5s - loss: 0.0480 - categorical_accuracy: 0.9860\n",
      "Epoch 32/50\n",
      " - 5s - loss: 0.0340 - categorical_accuracy: 0.9904\n",
      "Epoch 33/50\n",
      " - 6s - loss: 0.0234 - categorical_accuracy: 0.9938\n",
      "Epoch 34/50\n",
      " - 6s - loss: 0.0132 - categorical_accuracy: 0.9974\n",
      "Epoch 35/50\n",
      " - 5s - loss: 0.0084 - categorical_accuracy: 0.9985\n",
      "Epoch 36/50\n",
      " - 6s - loss: 0.0073 - categorical_accuracy: 0.9988\n",
      "Epoch 37/50\n",
      " - 6s - loss: 0.0061 - categorical_accuracy: 0.9992\n",
      "Epoch 38/50\n",
      " - 6s - loss: 0.0048 - categorical_accuracy: 0.9994\n",
      "Epoch 39/50\n",
      " - 6s - loss: 0.0033 - categorical_accuracy: 0.9995\n",
      "Epoch 40/50\n",
      " - 6s - loss: 0.0042 - categorical_accuracy: 0.9994\n",
      "Epoch 41/50\n",
      " - 6s - loss: 0.0041 - categorical_accuracy: 0.9995\n",
      "Epoch 42/50\n",
      " - 7s - loss: 0.0046 - categorical_accuracy: 0.9995\n",
      "Epoch 43/50\n",
      " - 7s - loss: 0.0043 - categorical_accuracy: 0.9993\n",
      "Epoch 44/50\n",
      " - 8s - loss: 0.0059 - categorical_accuracy: 0.9991\n",
      "Epoch 45/50\n",
      " - 8s - loss: 0.0044 - categorical_accuracy: 0.9992\n",
      "Epoch 46/50\n",
      " - 6s - loss: 0.0041 - categorical_accuracy: 0.9993\n",
      "Epoch 47/50\n",
      " - 5s - loss: 0.0048 - categorical_accuracy: 0.9992\n",
      "Epoch 48/50\n",
      " - 5s - loss: 0.0059 - categorical_accuracy: 0.9991\n",
      "Epoch 49/50\n",
      " - 8s - loss: 0.0046 - categorical_accuracy: 0.9992\n",
      "Epoch 50/50\n",
      " - 7s - loss: 0.0046 - categorical_accuracy: 0.9994\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1a257d33c8>"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = keras.models.Sequential()\n",
    "model.add(Dense(300,input_dim = numfeat,activation = 'relu'))\n",
    "model.add(Dense(500,activation = 'relu'))\n",
    "model.add(Dense(400,activation = 'relu'))\n",
    "model.add(Dense(20,activation='softmax'))\n",
    "model.compile(loss = 'categorical_crossentropy',optimizer = 'adam',metrics = ['categorical_accuracy'])\n",
    "model.fit(X_trainNN,y_trainNN,epochs=50,shuffle=True, verbose =2,batch_size=500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model needs to know what input shape it should expect. For this reason, the first layer in a Sequential model (and only the first, because following layers can do automatic shape inference) needs to receive information about its input shape. There are several possible ways to do this:\n",
    "\n",
    "Pass an input_shape argument to the first layer. This is a shape tuple (a tuple of integers or None entries, where None indicates that any positive integer may be expected). In input_shape, the batch dimension is not included.\n",
    "\n",
    "Some 2D layers, such as Dense, support the specification of their input shape via the argument input_dim, and some 3D temporal layers support the arguments input_dim and input_length.\n",
    "\n",
    "If you ever need to specify a fixed batch size for your inputs (this is useful for stateful recurrent networks), you can pass a batch_size argument to a layer. If you pass both batch_size=32 and input_shape=(6, 8) to a layer, it will then expect every batch of inputs to have the batch shape (32, 6, 8)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7955/7955 [==============================] - 1s 160us/step\n",
      "Accuracy with KERAS 0.7683218101597972\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy with KERAS\" ,model.evaluate(X_testNN,y_testNN)[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I have trained with KERAS on my pc for few times and achieved max accuracy of 0.81.\n",
    "\n",
    "Now, we have achieved almost similar accuracies in all the above models, I dont prefer NN's on this data as it is computationally very expensive."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PREDICTION\n",
    "\n",
    "\n",
    "I prefer just using the logisticRegression or linearsvm for predictions, but linearSVC also has almost same results. I'm not predict using Keras or Tensorflow, since it needs an extra two steps to convert the labels, which I dont want to waste my time on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/sklearn/svm/base.py:931: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LinearSVC(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "     intercept_scaling=1, loss='squared_hinge', max_iter=1500,\n",
       "     multi_class='crammer_singer', penalty='l2', random_state=0,\n",
       "     tol=0.0001, verbose=0)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "linearsvmfinal = LinearSVC(C=1.0,random_state=0,multi_class='crammer_singer',dual = False, max_iter = 1500)\n",
    "linearsvmfinal.fit(features,labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/lightgbm/__init__.py:46: UserWarning: Starting from version 2.2.1, the library file in distribution wheels for macOS is built by the Apple Clang (Xcode_8.3.3) compiler.\n",
      "This means that in case of installing LightGBM from PyPI via the ``pip install lightgbm`` command, you don't need to install the gcc compiler anymore.\n",
      "Instead of that, you need to install the OpenMP library, which is required for running LightGBM on the system with the Apple Clang compiler.\n",
      "You can install the OpenMP library by the following command: ``brew install libomp``.\n",
      "  \"You can install the OpenMP library by the following command: ``brew install libomp``.\", UserWarning)\n"
     ]
    }
   ],
   "source": [
    "import lightgbm as lgb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,\n",
       "        importance_type='split', learning_rate=0.1, max_depth=-1,\n",
       "        min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,\n",
       "        n_estimators=10000, n_jobs=-1, num_leaves=512,\n",
       "        objective='mutliclass', random_state=None, reg_alpha=0.0,\n",
       "        reg_lambda=0.0, silent=True, subsample=1.0,\n",
       "        subsample_for_bin=200000, subsample_freq=0)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gbm = lgb.LGBMClassifier(objective=\"mutliclass\",n_estimators=10000,num_leaves=512)\n",
    "gbm.fit(X_train,y_train,verbose = 300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = gbm.predict(testfeatures)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = linearsvmfinal.predict(testfeatures)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "predconv = encoder.inverse_transform(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub = pd.DataFrame({'id':testset.id,'cuisine':predconv})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = sub[['id','cuisine']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "output.to_csv(\"outputfile.csv\",index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
